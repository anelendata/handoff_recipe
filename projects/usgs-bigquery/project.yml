version: 0.3
description: Fetch the earth quake data from USGS.gov and store in BigQuery

installs:
- venv: proc_01
  command: pip install tap-rest-api
- venv: proc_02
  command: pip install target-bigquery-partition

vars:
- key: gcp_project_id
  value: "<your-gcp-project-id>"
- key: dataset_id
  value: usgs
- key: min_magnitude
  description: Minimum magnitude filter. This is referred in files/tap_config.json
  value: 1

envs:
- key: GOOGLE_APPLICATION_CREDENTIALS
  value: files/google_client_secret.json

tasks:
- name: make_tap_rest_api_schema
  description: Update schema and catalog for tap-rest-api to pull repos
  active: False  # Need to run only once locally to populate <project_dir>/flies
  commands:
  - command: "tap-rest-api --config files/tap_config_repo.json --schema_dir files/schema --catalog_dir files/catalog --infer_schema"
    venv: proc_01
  - command: "cp files/schema/*.json ../files/schema/"
  - command: "cp files/catalog/*.json ../files/catalog/"

- name: sync
  description: Do the sync
  pipeline:
  - command: "tap-rest-api files/custom_spec.json --config files/tap_config.json --schema_dir files/schema --catalog_dir files/catalog --catalog files/catalog/earthquakes.json --start_datetime {{ start_datetime }} --end_datetime {{ end_datetime }}"
    venv: proc_01
  - command: "target-bigquery --config files/target_config.json"
    venv: proc_02

deploy:
  cloud_provider: aws
  cloud_platform: fargate
  resource_group: handoff-etl
  container_image: tap-rest-api-target-bigquery
  task: usgs-earthquakes

schedules:
- target_id: daily
  description: Run everyday at 00:00:00Z
  envs:
  - key: __VARS
    value: 'start_datetime=$(date -Iseconds -d "00:00 yesterday") end_datetime=$(date -Iseconds -d "00:00 today")'
  cron: '0 0 * * ? *'

- target_id: hourly
  description: Run hour
  envs:
  - key: __VARS
    value: 'start_datetime=$(date -Ihours -d "1 hour ago") end_datetime=$(date -Ihours)'
  cron: '0 * * * ? *'
