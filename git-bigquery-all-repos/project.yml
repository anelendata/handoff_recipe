version: 0.3.0

description: Replicate GitHub data from all repos in the account to BigQuery

installs:
- venv: proc_01
  command: pip install --no-cache-dir tap-rest-api
- venv: proc_01
  command: pip install PyYAML
- venv: proc_02
  command: pip install --no-cache-dir https://github.com/anelendata/tap-github/archive/ba8a1465ed85458cf072f9aeee0247ac16b161cf.tar.gz#egg=tap-github
- venv: proc_03
  command: pip install --no-cache-dir https://github.com/anelendata/target-bigquery/archive/71b51aa8128d7b50a8155f6d9974308cd1d4c2d4.tar.gz#egg=target-bigquery

envs:
- key: GOOGLE_APPLICATION_CREDENTIALS
  value: files/google_client_secret.json  # value is retrieved from secure parameter store

vars:
- key: gcp_project_id
  value: my-gcp-project-id  # Change me!
- key: dataset_id
  value: my-bigquery-dataset-id  # Change me!
- key: historical_sync_start_at
  value: "1970-01-01T00:00:00Z"

tasks:
- name: make_tap_rest_api_schema
  description: Update schema and catalog for tap-rest-api to pull repos
  active: False  # Need to run only once locally to populate <project_dir>/flies
  commands:
  - command: tap-rest-api
    args: "--config files/tap_config_repo.json --schema_dir files/schema --catalog_dir files/catalog --infer_schema"
    venv: proc_01
  - command: cp
    args: "files/schema/*.json ../files/schema/"
  - command: cp
    args: "files/catalog/*.json ../files/catalog/"

- name: generate_tap_github_config_and_state
  description: "Generate tap-github config files per repo. Creates state file per repo if not exist in artifacts directory. These 3 commands are piped like: tap-rest_api | python ... | cat ..."
  pipeline:
  - command: tap-rest-api
    args: "--config files/tap_config_repo.json --schema_dir files/schema --catalog files/catalog/repository.json"
    venv: proc_01
  - command: python
    description: "{{ token }} is defined in .secret/secret.yml."
    args: "files/transformer/tap_config_github.py {{ token }} files/tables.yml {{ historical_sync_start_at }} files/tap_github_config artifacts/state"
    venv: proc_01
  - command: cat
    description: Save the list of repo full names (org/repo)
    args: "| sort | uniq > files/repos.txt"

- name: tap-github_discover_mode
  description: "Generate properties.json file for https://github.com/singer-io/tap-github"
  active: False  # Need to run only once locally to populate <project_dir>/flies
  pipeline:
  - command: head
    description: Get the first repo full name as we only need data from 1 repo to make the schema
    args: "-n 1 ./files/repos.txt" 
  - foreach:
    - name: generate_properties
      pipeline:
      - command: tap-github
        args: "--config files/tap_github_config_{{ _line }}.json --discover"
        venv: proc_02
      - command: cat
        args: "> files/properties.json"
    - name: activate_properties
      description: Mark the tables of interst with select=True
      pipeline:
      - command: python
        args: "files/transformer/properties.py files/properties.json ../files/properties.json files/tables.yml"
        venv: proc_01

- name: sync_all_repos
  description: Do the sync. It goes repo by repo to ensure sync completion happens 1 repo at a time just in case of GitHub rate limit. It leaves state files per repo so the sync becomes incremental once the historical sync completes.
  pipeline:
  - command: cat
    args: "./files/repos.txt"
    # foreach creates independent tasks per stdin chunk and loop through...
  - foreach:
    - name: extract
      description: tap (extract) the data and write out to an intermediate file
      pipeline:
      - command: tap-github
        args: "--config files/tap_github_config_{{ _line }}.json --properties files/properties.json --state artifacts/state_{{ _line }}.json"
        venv: proc_02
      - command: python
        description: A transformation layer to clean up the output from tap-github but it may not be necessary.
        args: "files/transformer/records.py files/properties.json"
        venv: proc_01
      - command: cat
        args: "> files/records_{{ _line }}.json"
    - name: load
      description: Load the records only if tap completes successfully
      pipeline:
      - command: cat
        args: files/records_{{ _line }}.json
      - command: target-bigquery
        descriptoin: Upload to BigQuery. It adds _sdc_batched_at that indicates the data load timestamp
        args: --config files/target_config.json
        venv: proc_03
    - name: copy_bookmark
      description: stdout from tap-bigquery is the bookmark. Make sure we save it and use it for the next run.
      pipeline:
      - command: mv
        args: artifacts/load_{{ _line }}_stdout.log artifacts/state_{{ _line }}.json

deploy:
  provider: aws
  platform: fargate
  resource_group: handoff-etl
  # Docker image name
  container_image: github-bigquery-all-repos
  # Fargate task name
  task: github-bigquery-sync

schedules:
- target_id: "1"
  description: Run every 8 hours
  cron: '0 */8 * * ? *'
